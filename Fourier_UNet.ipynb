{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb26a466-944f-4beb-9e36-698116118f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n",
      "True\n",
      "cuda:0\n",
      "NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "ngpu= 1\n",
    "print(torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\"))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "from models.unet_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce61343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from complexPyTorch.complexLayers import *\n",
    "from complexPyTorch.complexFunctions import *\n",
    "from models.unet_parts import *\n",
    "from models.unet_models import *\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class complexconv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, stride=1, groups=1):\n",
    "        super(complexconv, self).__init__()\n",
    "        self.conv = ComplexConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride, groups=groups)\n",
    "        self.bn = ComplexBatchNorm2d(out_channels)\n",
    "        self.relu = ComplexReLU()\n",
    "        self.leakyrelu = ComplexLeakyReLU()\n",
    "        self.rrelu = ComplexRReLU()\n",
    "        self.prelu = ComplexPReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        # x = self.leakyrelu(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class actlayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actlayer, self).__init__()\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        a, p = x.chunk(2, 1)\n",
    "        return torch.cat((self.relu(a), self.tanh(p)), dim=1)\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, stride=1, groups=1):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, stride=1, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.lrelu = nn.LeakyReLU(inplace=True)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.act = actlayer()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class FUnet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(FUnet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.fft = FourierTransform_Coe()\n",
    "\n",
    "        self.conv1 = complexconv(in_channels=3, out_channels=8, groups=1)\n",
    "        self.conv2 = complexconv(in_channels=8, out_channels=8, groups=1)\n",
    "        # self.inc = DoubleConv(n_channels, 8)\n",
    "\n",
    "        self.down1 = Down(8, 16)\n",
    "        self.down2 = Down(16, 32)\n",
    "        self.down3 = Down(32, 64)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(64, 128 // factor)\n",
    "        self.up1 = Up(128, 64 // factor, bilinear)\n",
    "        self.up2 = Up(64, 32 // factor, bilinear)\n",
    "        self.up3 = Up(32, 16 // factor, bilinear)\n",
    "        self.up4 = Up(16, 8, bilinear)\n",
    "        self.outc = OutConv(8, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x1 = self.inc(x)\n",
    "        x = self.fft.transform(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x1 = self.fft.inverse(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb58164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─ffn: 1-1                                    [-1, 3, 224, 224]         --\n",
      "|    └─ffe: 2-1                               [-1, 8, 224, 224]         --\n",
      "|    |    └─complexconv: 3-1                  [-1, 8, 224, 224]         489\n",
      "|    └─MaxPool2d: 2-2                         [-1, 3, 112, 112]         --\n",
      "|    └─ffe: 2-3                               [-1, 8, 112, 112]         (recursive)\n",
      "|    |    └─complexconv: 3-2                  [-1, 8, 112, 112]         (recursive)\n",
      "|    └─Upsample: 2-4                          [-1, 8, 224, 224]         --\n",
      "|    └─MaxPool2d: 2-5                         [-1, 3, 56, 56]           --\n",
      "|    └─ffe: 2-6                               [-1, 8, 56, 56]           (recursive)\n",
      "|    |    └─complexconv: 3-3                  [-1, 8, 56, 56]           (recursive)\n",
      "|    └─Upsample: 2-7                          [-1, 8, 112, 112]         --\n",
      "|    └─Upsample: 2-8                          [-1, 8, 224, 224]         --\n",
      "|    └─Conv2d: 2-9                            [-1, 3, 224, 224]         75\n",
      "├─tini_unet: 1-2                              [-1, 2, 224, 224]         --\n",
      "|    └─Sequential: 2-10                       [-1, 16, 224, 224]        --\n",
      "|    |    └─ComplexConv2d: 3-4                [-1, 16, 224, 224]        896\n",
      "|    |    └─NaiveComplexBatchNorm2d: 3-5      [-1, 16, 224, 224]        64\n",
      "|    |    └─ComplexLeakyReLU: 3-6             [-1, 16, 224, 224]        --\n",
      "|    |    └─ComplexConv2d: 3-7                [-1, 16, 224, 224]        4,640\n",
      "|    |    └─NaiveComplexBatchNorm2d: 3-8      [-1, 16, 224, 224]        64\n",
      "|    |    └─ComplexLeakyReLU: 3-9             [-1, 16, 224, 224]        --\n",
      "|    └─Sequential: 2-11                       [-1, 16, 112, 112]        --\n",
      "|    |    └─ComplexConv2d: 3-10               [-1, 16, 112, 112]        2,080\n",
      "|    |    └─NaiveComplexBatchNorm2d: 3-11     [-1, 16, 112, 112]        64\n",
      "|    |    └─ComplexLeakyReLU: 3-12            [-1, 16, 112, 112]        --\n",
      "|    └─Sequential: 2-12                       [-1, 32, 112, 112]        --\n",
      "|    |    └─ComplexConv2d: 3-13               [-1, 32, 112, 112]        9,280\n",
      "|    |    └─NaiveComplexBatchNorm2d: 3-14     [-1, 32, 112, 112]        128\n",
      "|    |    └─ComplexLeakyReLU: 3-15            [-1, 32, 112, 112]        --\n",
      "|    |    └─ComplexConv2d: 3-16               [-1, 32, 112, 112]        18,496\n",
      "|    |    └─NaiveComplexBatchNorm2d: 3-17     [-1, 32, 112, 112]        128\n",
      "|    |    └─ComplexLeakyReLU: 3-18            [-1, 32, 112, 112]        --\n",
      "|    └─Sequential: 2-13                       [-1, 32, 224, 224]        --\n",
      "|    |    └─ComplexConvTranspose2d: 3-19      [-1, 32, 224, 224]        32,832\n",
      "|    |    └─NaiveComplexBatchNorm2d: 3-20     [-1, 32, 224, 224]        128\n",
      "|    |    └─ComplexLeakyReLU: 3-21            [-1, 32, 224, 224]        --\n",
      "|    └─Sequential: 2-14                       [-1, 16, 224, 224]        --\n",
      "|    |    └─ComplexConv2d: 3-22               [-1, 16, 224, 224]        13,856\n",
      "|    |    └─NaiveComplexBatchNorm2d: 3-23     [-1, 16, 224, 224]        64\n",
      "|    |    └─ComplexLeakyReLU: 3-24            [-1, 16, 224, 224]        --\n",
      "|    |    └─ComplexConv2d: 3-25               [-1, 16, 224, 224]        4,640\n",
      "|    |    └─NaiveComplexBatchNorm2d: 3-26     [-1, 16, 224, 224]        64\n",
      "|    |    └─ComplexLeakyReLU: 3-27            [-1, 16, 224, 224]        --\n",
      "|    └─Conv2d: 2-15                           [-1, 2, 224, 224]         34\n",
      "===============================================================================================\n",
      "Total params: 88,022\n",
      "Trainable params: 88,022\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 6.44\n",
      "===============================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 185.66\n",
      "Params size (MB): 0.34\n",
      "Estimated Total Size (MB): 186.57\n",
      "===============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "├─ffn: 1-1                                    [-1, 3, 224, 224]         --\n",
       "|    └─ffe: 2-1                               [-1, 8, 224, 224]         --\n",
       "|    |    └─complexconv: 3-1                  [-1, 8, 224, 224]         489\n",
       "|    └─MaxPool2d: 2-2                         [-1, 3, 112, 112]         --\n",
       "|    └─ffe: 2-3                               [-1, 8, 112, 112]         (recursive)\n",
       "|    |    └─complexconv: 3-2                  [-1, 8, 112, 112]         (recursive)\n",
       "|    └─Upsample: 2-4                          [-1, 8, 224, 224]         --\n",
       "|    └─MaxPool2d: 2-5                         [-1, 3, 56, 56]           --\n",
       "|    └─ffe: 2-6                               [-1, 8, 56, 56]           (recursive)\n",
       "|    |    └─complexconv: 3-3                  [-1, 8, 56, 56]           (recursive)\n",
       "|    └─Upsample: 2-7                          [-1, 8, 112, 112]         --\n",
       "|    └─Upsample: 2-8                          [-1, 8, 224, 224]         --\n",
       "|    └─Conv2d: 2-9                            [-1, 3, 224, 224]         75\n",
       "├─tini_unet: 1-2                              [-1, 2, 224, 224]         --\n",
       "|    └─Sequential: 2-10                       [-1, 16, 224, 224]        --\n",
       "|    |    └─ComplexConv2d: 3-4                [-1, 16, 224, 224]        896\n",
       "|    |    └─NaiveComplexBatchNorm2d: 3-5      [-1, 16, 224, 224]        64\n",
       "|    |    └─ComplexLeakyReLU: 3-6             [-1, 16, 224, 224]        --\n",
       "|    |    └─ComplexConv2d: 3-7                [-1, 16, 224, 224]        4,640\n",
       "|    |    └─NaiveComplexBatchNorm2d: 3-8      [-1, 16, 224, 224]        64\n",
       "|    |    └─ComplexLeakyReLU: 3-9             [-1, 16, 224, 224]        --\n",
       "|    └─Sequential: 2-11                       [-1, 16, 112, 112]        --\n",
       "|    |    └─ComplexConv2d: 3-10               [-1, 16, 112, 112]        2,080\n",
       "|    |    └─NaiveComplexBatchNorm2d: 3-11     [-1, 16, 112, 112]        64\n",
       "|    |    └─ComplexLeakyReLU: 3-12            [-1, 16, 112, 112]        --\n",
       "|    └─Sequential: 2-12                       [-1, 32, 112, 112]        --\n",
       "|    |    └─ComplexConv2d: 3-13               [-1, 32, 112, 112]        9,280\n",
       "|    |    └─NaiveComplexBatchNorm2d: 3-14     [-1, 32, 112, 112]        128\n",
       "|    |    └─ComplexLeakyReLU: 3-15            [-1, 32, 112, 112]        --\n",
       "|    |    └─ComplexConv2d: 3-16               [-1, 32, 112, 112]        18,496\n",
       "|    |    └─NaiveComplexBatchNorm2d: 3-17     [-1, 32, 112, 112]        128\n",
       "|    |    └─ComplexLeakyReLU: 3-18            [-1, 32, 112, 112]        --\n",
       "|    └─Sequential: 2-13                       [-1, 32, 224, 224]        --\n",
       "|    |    └─ComplexConvTranspose2d: 3-19      [-1, 32, 224, 224]        32,832\n",
       "|    |    └─NaiveComplexBatchNorm2d: 3-20     [-1, 32, 224, 224]        128\n",
       "|    |    └─ComplexLeakyReLU: 3-21            [-1, 32, 224, 224]        --\n",
       "|    └─Sequential: 2-14                       [-1, 16, 224, 224]        --\n",
       "|    |    └─ComplexConv2d: 3-22               [-1, 16, 224, 224]        13,856\n",
       "|    |    └─NaiveComplexBatchNorm2d: 3-23     [-1, 16, 224, 224]        64\n",
       "|    |    └─ComplexLeakyReLU: 3-24            [-1, 16, 224, 224]        --\n",
       "|    |    └─ComplexConv2d: 3-25               [-1, 16, 224, 224]        4,640\n",
       "|    |    └─NaiveComplexBatchNorm2d: 3-26     [-1, 16, 224, 224]        64\n",
       "|    |    └─ComplexLeakyReLU: 3-27            [-1, 16, 224, 224]        --\n",
       "|    └─Conv2d: 2-15                           [-1, 2, 224, 224]         34\n",
       "===============================================================================================\n",
       "Total params: 88,022\n",
       "Trainable params: 88,022\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.44\n",
       "===============================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 185.66\n",
       "Params size (MB): 0.34\n",
       "Estimated Total Size (MB): 186.57\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from complexPyTorch.complexLayers import *\n",
    "from complexPyTorch.complexFunctions import *\n",
    "from models.unet_parts import *\n",
    "from models.unet_models import *\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class complexconv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, stride=1, groups=1):\n",
    "        super(complexconv, self).__init__()\n",
    "        self.conv = ComplexConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride, groups=groups)\n",
    "        self.bn = ComplexBatchNorm2d(out_channels)\n",
    "        self.relu = ComplexReLU()\n",
    "        self.leakyrelu = ComplexLeakyReLU()\n",
    "        self.rrelu = ComplexRReLU()\n",
    "        self.prelu = ComplexPReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        # x = self.leakyrelu(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class ffe(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ffe, self).__init__()\n",
    "        self.n_channels = in_channels\n",
    "        self.n_classes = out_channels\n",
    "        self.fft = FourierTransform_Coe()\n",
    "        self.conv = complexconv(in_channels=in_channels, out_channels=out_channels)\n",
    "    def forward(self, x):\n",
    "        x = self.fft.transform(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.fft.inverse(x)\n",
    "        return x\n",
    "\n",
    "class ffn(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super(ffn,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.mid_channels = mid_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.fblock = ffe(in_channels=self.in_channels, out_channels=self.mid_channels)\n",
    "        self.conv = nn.Conv2d(in_channels=self.mid_channels*3, out_channels=self.out_channels, kernel_size=1)\n",
    "        self.down = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "    def forward(self, x):\n",
    "        c1 = self.fblock(x)\n",
    "\n",
    "        x2 = self.down(x)\n",
    "        c2 = self.fblock(x2)\n",
    "        c2 = self.up(c2)\n",
    "\n",
    "        x3 = self.down(x2)\n",
    "        c3 = self.fblock(x3)\n",
    "        c3 = self.up(c3)\n",
    "        c3 = self.up(c3)\n",
    "\n",
    "        # x4 = self.down(x3)\n",
    "        # c4 = self.fblock(x4)\n",
    "        # c4 = self.up(c4)\n",
    "        # c4 = self.up(c4)\n",
    "        # c4 = self.up(c4)\n",
    "\n",
    "        c = torch.cat([c1, c2, c3], 1)\n",
    "        x = self.conv(c)\n",
    "        return x\n",
    "class fcn(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(fcn, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.ffn = ffn(in_channels=3, mid_channels=8, out_channels=n_channels)\n",
    "        self.tini_unet = tini_unet(n_channels=n_channels, n_classes=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        saliency_map = self.ffn(x)\n",
    "        logits = self.tini_unet(x)\n",
    "        return logits, saliency_map\n",
    "\n",
    "model = fcn(3, 2).cuda()\n",
    "summary(model, (3, 224, 224), dtypes=[torch.float])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1b9f5",
   "metadata": {},
   "source": [
    "## model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe195e-8529-48e5-9099-19e6275f4733",
   "metadata": {},
   "source": [
    "<p>繪製 Model:<p>\n",
    "\n",
    "<pre><code>from torchviz import make_dot\n",
    "x = torch.randn(1, 3, 256, 256).requires_grad_(True).cuda()\n",
    "y = model(x)\n",
    "vis_graph = make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))\n",
    "vis_graph.view()\n",
    "</code></pre>\n",
    "\n",
    "<p>輸出 Model parameters:\n",
    "<pre><code> \n",
    "model = UNet(n_class=3).cuda()\n",
    "for name, param in model.named_parameters():\n",
    "     print(name, param.shape)\n",
    "</code></pre>\n",
    "\n",
    "<pre><code>\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68ab6f-8967-4e17-a376-944aff04ccd6",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "tr_img_folder = os.path.join(r'C:\\Users\\user\\pythonProject\\mission87\\data\\DUTS\\DUTS-TR\\DUTS-TR-Image')\n",
    "tr_gt_folder = os.path.join(r'C:\\Users\\user\\pythonProject\\mission87\\data\\DUTS\\DUTS-TR\\DUTS-TR-Mask')\n",
    "\n",
    "val_percent = 0.0\n",
    "batch_size = 8\n",
    "\n",
    "train_set = DatasetGenerate(tr_img_folder, tr_gt_folder, phase='train', test_size=0.1)\n",
    "val_set = DatasetGenerate(tr_img_folder, tr_gt_folder, phase='val', test_size=0.1)\n",
    "\n",
    "loader_args = dict(batch_size=batch_size, num_workers=4, prefetch_factor =2 , pin_memory=True)\n",
    "train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_loader,\n",
    "    'val': val_loader\n",
    "}\n",
    "\n",
    "from itertools import chain\n",
    "optimizer = torch.optim.Adam(params=chain(model.ffn.parameters(),\n",
    "                                          model.tini_unet.parameters()),\n",
    "                            lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3df74",
   "metadata": {},
   "source": [
    "### MSRA10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataloader import *\n",
    "# import torchvision\n",
    "# from torchvision import transforms, datasets, models\n",
    "# import os\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "# # MSRA10K 沒有預先分類 train, test dataset 因此多一步處理\n",
    "# tr_img_folder = os.path.join(r'C:\\Users\\User\\pythonProject\\mission87\\data\\MSRA10K_Imgs_GT\\Images')\n",
    "# tr_gt_folder = os.path.join(r'C:\\Users\\User\\pythonProject\\mission87\\data\\MSRA10K_Imgs_GT\\Mask')\n",
    "\n",
    "# val_percent = 0.1\n",
    "# batch_size = 8\n",
    "\n",
    "# tr_set = DatasetGenerate(tr_img_folder, tr_gt_folder, phase='train', test_size=0.1)\n",
    "# ts_set = DatasetGenerate(tr_img_folder, tr_gt_folder, phase='val', test_size=0.1)\n",
    "# n_val = int(len(tr_set) * val_percent)\n",
    "# n_train = len(tr_set) - n_val\n",
    "# train_set, val_set = random_split(tr_set, [n_train, n_val], generator=torch.Generator().manual_seed(0)) # MSRA10K: [train_set, test_set], DUTS: [train_set, val_set]\n",
    "\n",
    "# loader_args = dict(batch_size=batch_size, num_workers=4, prefetch_factor =2 , pin_memory=True)\n",
    "# train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
    "# val_loader = DataLoader(val_set, shuffle=False, drop_last=True, **loader_args)\n",
    "\n",
    "# dataloaders = {\n",
    "#     'train': train_loader,\n",
    "#     'val': val_loader\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f56643-155a-436d-a694-051e8d3427d1",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19aad67-cfe3-4cc3-b734-8342cc7792f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model : {unet, complexunet, amlitude_phase}\n",
    "phase: {}\n",
    "lossfunction: {MSE, FFL, BCE, BCE-Dice}\n",
    "'''\n",
    "from train import train_model\n",
    "# model = testunet16(n_channels=8, n_classes=3)\n",
    "# model = amlitude_phase(3, 3)\n",
    "# model = fcn(3, 2)\n",
    "# summary(model, (1, 224, 224), dtypes=[torch.float])\n",
    "model = train_model(model=model, dataloaders=dataloaders, optimizer=optimizer, num_epochs=50, phase='spatial domain', task='FFNv2', lossfunction='BCE-Dice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fbbb5",
   "metadata": {},
   "source": [
    "# Test loss func. & dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159bc8b-f2cc-4ee5-a551-7c7bd429463a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## test loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a94568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.unet_models import *\n",
    "model = UNet(n_channels=3, n_classes=3).cuda()\n",
    "# summary(model, (3, 224, 224), dtypes=[torch.float])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch = next(iter(dataloaders['train']))\n",
    "image = batch['image'].to(device=device, dtype=torch.float32)\n",
    "mask = batch['mask'].to(device=device, dtype=torch.long)\n",
    "\n",
    "pred = model(image.to(torch.float))\n",
    "\n",
    "criterion(pred, mask[:,-1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6666960",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BceDiceLoss(multiclass=True, n_classes=model.n_classes)\n",
    "criterion(pred, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.):\n",
    "    \"\"\"Dice loss\n",
    "    \"\"\"\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "class Weighted_Cross_Entropy_Loss(torch.nn.Module):\n",
    "    \"\"\"Cross entropy loss that uses weight maps.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Weighted_Cross_Entropy_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target, weights=10):\n",
    "        n, c, H, W = pred.shape\n",
    "        # # Calculate log probabilities\n",
    "        logp = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        # Gather log probabilities with respect to target\n",
    "        logp = torch.gather(logp, 1, target.view(n, 1, H, W))\n",
    "\n",
    "        # Multiply with weights\n",
    "        weighted_logp = (logp * weights).view(n, -1)\n",
    "\n",
    "        # Rescale so that loss is in approx. same interval\n",
    "        weighted_loss = weighted_logp.sum(1) / weights.view(n, -1).sum(1)\n",
    "\n",
    "        # Average over mini-batch\n",
    "        weighted_loss = -weighted_loss.mean()\n",
    "\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686feb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Weighted_Cross_Entropy_Loss()\n",
    "loss = criterion(pred, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "loss(F.sigmoid(pred), mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "softmax(pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c8cad-a542-468b-abd8-e32784da0bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn([1, 3, 8, 8], requires_grad=True)\n",
    "test = test.to(torch.cfloat) #dtype to complex64\n",
    "test = test.cuda()\n",
    "# print(test)\n",
    "# print(test)\n",
    "\n",
    "pred = torch.ones([1, 3, 8, 8], requires_grad=True)\n",
    "pred = pred.to(torch.cfloat) #dtype to complex64\n",
    "pred = pred.cuda()\n",
    "# print(pred)\n",
    "\n",
    "mse = MSELoss() #因實部, 虛部拆分計算, 因此 n 會 *2\n",
    "loss = mse(pred, test)\n",
    "print('custom mse function', loss)\n",
    "loss.backward()\n",
    "\n",
    "print('torch native mse funciton', torch.nn.functional.mse_loss(pred.to(torch.float), test.to(torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_freq = torch.stack([pred.real, pred.imag], -1)\n",
    "target_freq = torch.stack([test.real, test.imag], -1)\n",
    "print(pred_freq, target_freq, pred_freq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f46a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_distance = (pred_freq - target_freq) ** 2\n",
    "print(freq_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46cc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(freq_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b57d3-74a9-4473-b694-94fd750284e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## check training/valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03859ba8-3274-43fd-9a29-7fe468b3fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch = next(iter(dataloaders['train']))\n",
    "image = batch['image']\n",
    "mask = batch['mask']\n",
    "\n",
    "s = {0: 'orignal image & mask',1: 'Fourier coefficients (with mask)', 2: 'Fourier coefficients (with image*mask)', 3: 'Fourier amplitude', 4: 'Fourier phase'}\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "fig.suptitle('orignal image & mask', verticalalignment='bottom')\n",
    "ax1.imshow(image[0].permute(1, 2, 0), 'gray')\n",
    "ax1.set_title('image')\n",
    "ax2.imshow(mask[0].permute(1, 2, 0)*255, 'gray')\n",
    "ax2.set_title('mask')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(image.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dde2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(o[0].permute(1, 2, 0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c036bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.unet_parts import FFT2\n",
    "f = FFT2()\n",
    "am, p = f.transform(image[0])\n",
    "i = f.inverse(am, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01364570",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.log(am).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43996166",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a561324",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.fft.fft2(image[0])\n",
    "fshift = np.fft.fftshift(f)\n",
    "amplitude = np.absolute(fshift)\n",
    "phase = np.angle(fshift)\n",
    "\n",
    "ishift = np.fft.ifftshift(amplitude*np.exp(1j*phase))\n",
    "iimg = np.fft.ifft2(ishift)\n",
    "iimg = np.absolute(iimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115668db",
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0] = image[0] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b177b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(amplitude).transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(iimg.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d54eb53-4772-4976-88e7-a582a18183ea",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e02cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test_unet eval\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import *\n",
    "from PIL import Image\n",
    "# model = torch.load(r'C:\\Users\\User\\pytho/nProject\\mission87\\task\\BCE unet\\net\\net-50.pkl').cuda()\n",
    "# model = torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\fcn-with-coe-v2\\net\\best.pkl')\n",
    "# model.load_state_dict(torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\fcn-with-coe-v3\\net\\best.pkl'))\n",
    "'''\n",
    "load model from task\n",
    "'''\n",
    "# model = torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\funet\\net.pkl')\n",
    "# model.load_state_dict(torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\funet\\net\\best.pkl'))\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(dataloaders['val']))\n",
    "    image = batch['image'].to(device=device, dtype=torch.float32)\n",
    "    mask = batch['mask'].to(device=device, dtype=torch.long)[:, -1, :, :]\n",
    "    pred = model(image)\n",
    "    for i in range(batch_size):\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4, figsize=(15, 5))\n",
    "        ax1.imshow((image[i]).permute(1, 2, 0).cpu().numpy())\n",
    "        ax1.set_title('image')\n",
    "        ax2.imshow(mask[i].cpu())\n",
    "        ax2.set_title('mask')\n",
    "        # ax3.imshow((pred[i]).permute(1, 2, 0).cpu().numpy(), 'gray') #BCE 沒有\n",
    "        # ax3.set_title('pred')\n",
    "        ax4.imshow(binary(pred[i], model.n_classes))\n",
    "        # ax4.imshow(objectmap(pred[i]))\n",
    "        ax4.set_title('binary pred')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "model.conv2.register_forward_hook(get_activation('fc3'))\n",
    "output = model(image)\n",
    "mid_output = activation['fc3']\n",
    "\n",
    "m_out = mid_output[0]\n",
    "f = FourierTransform_Coe()\n",
    "img = f.inverse(m_out) *255.0\n",
    "plt.imshow(img.permute(1, 2, 0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4, figsize=(15, 5))\n",
    "i = 7\n",
    "# ax1.imshow(pred[i].permute(1, 2, 0).cpu().numpy())\n",
    "ax1.set_title('pred')\n",
    "ax2.imshow(pred[i][0].cpu())\n",
    "ax2.set_title('pred[0]')\n",
    "ax3.imshow(pred[i][1].cpu()) #BCE 沒有\n",
    "ax3.set_title('pred[1]')\n",
    "ax4.imshow(pred[i][2].cpu())\n",
    "ax4.set_title('pred[2]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121bf051",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[5].permute(1, 2, 0).cpu())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "i = 5\n",
    "ax1.imshow(pred[i][0].cpu()) #BCE 沒有\n",
    "ax1.set_title('pred[1]')\n",
    "ax2.imshow(pred[i][1].cpu())\n",
    "ax2.set_title('pred[2]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred[5][1].cpu())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47515c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torchviz import make_dot\n",
    "batch = next(iter(dataloaders['train']))\n",
    "image = batch['image'].to(device=device, dtype=torch.float32)\n",
    "mask = batch['mask'].to(device=device, dtype=torch.long)[:, -1, :, :]\n",
    "pred = model(image)\n",
    "# yhat = model(batch.text) # Give dummy batch to forward().\n",
    "make_dot(pred, params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8df863",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pred[3]\n",
    "plt.hist(img.ravel().cpu().numpy(), bins=50, density=True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13abcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Image.open(r'C:\\Users\\User\\pythonProject\\mission87\\data\\DUTS\\DUTS-TR\\DUTS-TR-Image\\ILSVRC2012_test_00000004.jpg')\n",
    "p = np.array(p) / 255.0\n",
    "p = torch.as_tensor(p.copy()).float().contiguous().permute(2, 1, 0)\n",
    "p = p.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36589c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b984ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=2, padding=0, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = FFT()\n",
    "p_coe = fft.transform(p)\n",
    "p2 = fft.inverse(p_coe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = p2\n",
    "img -= torch.min(img)\n",
    "img /= torch.max(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pred\n",
    "p -= torch.min(p)\n",
    "p /= torch.max(p)\n",
    "plt.imshow(p[0].permute(1, 2, 0).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d7fbb",
   "metadata": {},
   "source": [
    "## 其他檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channels=3\n",
    "pred -= torch.min(pred)\n",
    "pred /= torch.max(pred)\n",
    "pred\n",
    "\n",
    "probs = torch.sigmoid(pred[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdfe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred -= torch.min(pred)\n",
    "pred /= torch.max(pred)\n",
    "pred\n",
    "# plot the pixel values\n",
    "plt.hist(pred[0].cpu().numpy().ravel(), bins=50, density=True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mask = pred[0].cpu().squeeze()\n",
    "mask = ((full_mask > 0.5).float()*1).numpy()\n",
    "result = Image.fromarray((mask * 255).astype(np.uint8).transpose(1, 2, 0))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "probs = torch.sigmoid(pred[2].unsqueeze(0))[0]\n",
    "full_mask = probs.cpu().squeeze()\n",
    "\n",
    "mask = F.one_hot(full_mask.argmax(dim=0), 3).permute(2, 0, 1).numpy()\n",
    "\n",
    "result = Image.fromarray((np.argmax(mask, axis=0) * 255 / mask.shape[0]).astype(np.uint8))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c84841",
   "metadata": {},
   "source": [
    "# Eval: test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63128c0-098b-4465-b679-0004352a77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils.utils import *\n",
    "te_img_folder = os.path.join(r'C:\\Users\\user\\pythonProject\\mission87\\data\\DUTS\\DUTS-TE\\DUTS-TE-Image')\n",
    "te_gt_folder = os.path.join(r'C:\\Users\\user\\pythonProject\\mission87\\data\\DUTS\\DUTS-TE\\DUTS-TE-Mask')\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataset = DatasetGenerate(te_img_folder, te_gt_folder)\n",
    "\n",
    "loader_args = dict(batch_size=batch_size, num_workers=4, prefetch_factor =2 , pin_memory=True)\n",
    "test_loader = DataLoader(dataset, shuffle=False, **loader_args)\n",
    "\n",
    "dataloaders = {\n",
    "    'test': test_loader\n",
    "}\n",
    "\n",
    "from models.unet_models import *\n",
    "# model = torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\BCE unet\\net\\net-50.pkl')\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\FFN\\net\\best.pkl'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch= next(iter(dataloaders['test']))\n",
    "    image = batch['image'].to(device=device, dtype=torch.float32)\n",
    "    mask = batch['mask'].to(device=device, dtype=torch.long)[:, -1, :, :]\n",
    "    pred = model(image)\n",
    "    for i in range(batch_size):\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "        ax1.imshow((image[i]).permute(1, 2, 0).cpu().numpy())\n",
    "        ax1.set_title('image')\n",
    "        ax2.imshow(mask[i].cpu())\n",
    "        ax2.set_title('mask')\n",
    "        ax3.imshow(binary(pred[i], model.n_classes))\n",
    "        # ax3.imshow(objectmap(pred[i]))\n",
    "        ax3.set_title('binary pred')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ef936",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_in_hook = []\n",
    "features_out_hook = []\n",
    "\n",
    "def hook(module, fea_in, fea_out):\n",
    "    features_in_hook.append(fea_in)\n",
    "    features_out_hook.append(fea_out.detach().cpu().numpy())\n",
    "    return None\n",
    "\n",
    "model._modules.get('conv').register_forward_hook(hook=hook)\n",
    "result = model(image)\n",
    "print(features_in_hook)  # 勾的是指定层的输入\n",
    "print(features_out_hook)  # 勾的是指定层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f91e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "torch.optim.Adam(params=chain(model.fblock.parameters(),\n",
    "                        model.conv.parameters(),\n",
    "                        model.unet.parameters()),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, features_in_forward, features_out_forward = model(image)\n",
    "print(\"*\"*5+\"forward return features\"+\"*\"*5)\n",
    "print(features_in_forward)\n",
    "print(features_out_forward)\n",
    "print(\"*\"*5+\"forward return features\"+\"*\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a964cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_modules('conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bab26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "i = 3\n",
    "ax1.imshow(pred[i][0].cpu()) #BCE 沒有\n",
    "ax1.set_title('pred[1]')\n",
    "ax2.imshow(pred[i][1].cpu())\n",
    "ax2.set_title('pred[2]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[i][0], pred[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6270614",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_output = torch.tensor([[[[0.3157, 0.7023], [0.7108, 0.3082]], [[0.6998, 0.3112], [0.3067, 0.7068]]]])\n",
    "print(pred_output)\n",
    "temp1 = F.softmax(pred_output,dim=1)\n",
    "print(\"temp1:\",temp1)\n",
    "\n",
    "temp3 = torch.log(temp1)\n",
    "print(\"temp3:\",temp3)\n",
    "\n",
    "target = torch.tensor([[[1,0],[0,1]]])\n",
    "target = target.long()\n",
    "loss1 = F.nll_loss(temp3,target)\n",
    "print('loss1: ', loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = binary(pred_output, model.n_classes)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01315a3c",
   "metadata": {},
   "source": [
    "# save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca26409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "from dataloader import *\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# te_img_folder = os.path.join(r'C:\\Users\\user\\pythonProject\\mission87\\data\\DUTS\\DUTS-TE\\DUTS-TE-Image')\n",
    "# te_gt_folder = os.path.join(r'C:\\Users\\user\\pythonProject\\mission87\\data\\DUTS\\DUTS-TE\\DUTS-TE-Mask')\n",
    "te_img_folder = os.path.join(r'C:\\Users\\User\\pythonProject\\mission87\\eval_outputs\\msra5')\n",
    "te_gt_folder = os.path.join(r'C:\\Users\\User\\pythonProject\\mission87\\eval_outputs\\mask-msra55')\n",
    "batch_size = 1\n",
    "\n",
    "dataset = DatasetGenerate(te_img_folder, te_gt_folder, test_size=0.0)\n",
    "\n",
    "loader_args = dict(batch_size=batch_size, num_workers=4, prefetch_factor =2 , pin_memory=True)\n",
    "loader_args = dict(batch_size=batch_size)\n",
    "test_loader = DataLoader(dataset, shuffle=False, **loader_args)\n",
    "\n",
    "dataloaders = {\n",
    "    'test': test_loader\n",
    "}\n",
    "\n",
    "\n",
    "root_path = r'C:\\Users\\User\\pythonProject\\mission87\\eval_outputs'\n",
    "pred_path = r'unet-msra'\n",
    "mask_path = r'mask-msra'\n",
    "\n",
    "p = pred_path\n",
    "# for p in p:\n",
    "if not os.path.exists(os.path.join(root_path, p)):\n",
    "    os.makedirs(os.path.join(root_path, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b87cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\unet-MSRA10K\\net\\best.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e58ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "# model = torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\BCE unet\\net\\net-50.pkl').cuda()\n",
    "# model = torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\BCE unet\\net\\net-3.pkl')\n",
    "# image_path = r'images'\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloaders['test']):\n",
    "        image = batch['image'].to(device=device, dtype=torch.float32)\n",
    "        mask = batch['mask'].to(device=device, dtype=torch.long)[:, -1, :, :]\n",
    "        pred = model(image)\n",
    "        pred__path = os.path.join(root_path, pred_path, str(i) + '.png')\n",
    "        Image_copy = Image.Image.copy(binary(pred[0], model.n_classes))\n",
    "        # Image_copy = Image.Image.copy(objectmap(pred[0]))\n",
    "        Image.Image.save(Image_copy, pred__path)\n",
    "        save_image(mask[0,:,:].to(device=device, dtype=torch.float), os.path.join(root_path, 'mask-msra', str(i) + '.png'))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0cb45e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 20.429ms Std: 0.588ms FPS: 48.95\n",
      "20.42941535949707\n"
     ]
    }
   ],
   "source": [
    "# model = torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\BCE unet\\net\\net-50.pkl').cuda()\n",
    "device = torch.device('cuda')\n",
    "model.to(device=device)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "#GPU-WARM-UP\n",
    "dummy_input = torch.randn(1, 3, 224, 224,dtype=torch.float).to(device)\n",
    "for _ in range(100):\n",
    "   _ = model(dummy_input)\n",
    "\n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "   for rep in range(repetitions):\n",
    "      starter.record()\n",
    "      _ = model(dummy_input)\n",
    "      ender.record()\n",
    "      # WAIT FOR GPU SYNC\n",
    "      torch.cuda.synchronize()\n",
    "      curr_time = starter.elapsed_time(ender)\n",
    "      timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "mean_fps = 1000. / mean_syn\n",
    "print('Mean: {mean_syn:.3f}ms Std: {std_syn:.3f}ms FPS: {mean_fps:.2f}'.format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))\n",
    "print(mean_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model = torch.load(r'C:\\Users\\User\\pythonProject\\mission87\\task\\BCE unet\\net\\net-50.pkl')\n",
    "device = torch.device('cpu')\n",
    "model.to(device=device)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "dummy_input = torch.randn(1, 3, 224, 224,dtype=torch.float).to(device)\n",
    "with torch.no_grad():\n",
    "   for rep in range(repetitions):\n",
    "      starter = time.perf_counter()\n",
    "      _ = model(dummy_input)\n",
    "      curr_time = time.perf_counter() - starter\n",
    "      timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "mean_fps = 1. / mean_syn\n",
    "print('Mean: {mean_syn:.3f}s Std: {std_syn:.3f}s FPS: {mean_fps:.2f}'.format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))\n",
    "print(mean_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = get_loader(te_img_folder, te_gt_folder, phase='test',\n",
    "#                           batch_size=8, shuffle=True, num_workers=4, prefetch_factor=2,\n",
    "#                           transform=test_transform)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     image, mask = next(iter(test_loader))\n",
    "#     image, mask = image.to(torch.float).cuda(), mask.to(torch.float).cuda()\n",
    "#     pred = model_1(image)\n",
    "#     pred2 = model_2(image)\n",
    "#     for i in range(8):\n",
    "#         image_ = (image[i]).permute(1, 2, 0).cpu().numpy()\n",
    "#         mask_ = (mask[i]).permute(1, 2, 0).cpu().numpy()\n",
    "#         pred_ = (pred[i]).permute(1, 2, 0).cpu().detach().numpy()\n",
    "#         pred_2 = (pred2[i]).permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "#         fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4, figsize=(15, 5))\n",
    "#         ax1.imshow(image_)\n",
    "#         ax1.set_title('image')\n",
    "#         ax2.imshow(mask_)\n",
    "#         ax2.set_title('mask')\n",
    "#         ax3.imshow(pred_)\n",
    "#         ax3.set_title('pred by orig')\n",
    "#         ax4.imshow(pred_2)\n",
    "#         ax4.set_title('pred by freq')\n",
    "#         plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "c457ca4d07b6df64aa658312f32bb07e9ca064137e49bc80b66c902c997caca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
